
import requests
from bs4 import BeautifulSoup
import csv
from tqdm import tqdm
from termcolor import colored
import re

from Url_Scraper import UrlScraper

class VulnerabilityScraper:
    def __init__(self, urls):
        self.urls = urls

    def scrape_vulnerabilities(self, num_urls=None):

        results = []
        with tqdm(total=num_urls,desc=colored("Scraping Data ", "yellow"),colour="GREEN") as pbar:


         for url in self.urls:
           
            response = requests.get(url)
            soup = BeautifulSoup(response.text, "html.parser")

        # Extract data from the webpage
            cve_pattern = r"CVE-\d{1,9}-\d{1,9}"
            try:
                cve= (re.search(cve_pattern, response.text)).group()
            except:
                cve= "N/A"


            cwe_pattern = r"CWE-\d{3,7}"
            try:
                cwe= (re.search(cwe_pattern, response.text)).group()
            except:
                cwe= "N/A"




        # ===========================================
            
            vulnerability_name = soup.find("h1", class_="vue--heading title").text.strip()
            # description = soup.find("div", class_="vuln-description").text.strip()
        #     # ...

        #     # Append the scraped data to the results list
            results.append([cve,cwe,url])  # Modify this line based on your extracted data

        #     # Update the progress bar
            pbar.update(1)

        return results

    def save_to_csv(self, filename):

        # create file if it doesn't exist
        
        results = self.scrape_vulnerabilities()

        with open(filename, "w", newline="", encoding="utf-8") as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(["CVE","CWE","URL"])  # Modify this line based on your extracted data
            writer.writerows(results)
        
        print(f"Data saved to {filename} Successfully.")




if __name__ == "__main__":

    # scrape urls
    
    base_url = "https://security.snyk.io/vuln/"
    num_pages = 30

    scrape = UrlScraper(base_url, num_pages)
    urls = scrape.scrape_vulnerability_urls()
    num_urls= len(urls)
    
    # scrape vulnerabilities
    scraper = VulnerabilityScraper(urls)
    scraper.scrape_vulnerabilities(num_urls)

    # save to csv
    scraper.save_to_csv("Vulnerabilities.csv")
